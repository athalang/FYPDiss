# %%
import os

os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"

# %%
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random

SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)
torch.use_deterministic_algorithms(True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# %%
VOCAB = {'a':0, 'b':1, 'c':2}
PAD_IDX = len(VOCAB)
VOCAB_SIZE = PAD_IDX + 1

# %%
class ModCountDataset(torch.utils.data.Dataset):
    def __init__(self, size, min_len, max_len):
        self.size = size
        self.min_len = min_len
        self.max_len = max_len
        self.tokens = VOCAB

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        length = random.randint(self.min_len, self.max_len)
        seq = random.choices(['a','b','c'], k=length)
        mod_a = seq.count('a') % 3
        mod_b = seq.count('b') % 2
        label = 1 if (mod_a == 0 and mod_b == 0) else 0
        seq_ids = torch.tensor([self.tokens[t] for t in seq])
        return seq_ids, torch.tensor(label)

# %%
def collate_fn(batch):
    sequences, labels = zip(*batch)
    lengths = [len(seq) for seq in sequences]
    padded_seqs = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PAD_IDX)
    labels = torch.stack(labels)
    return padded_seqs.to(device), labels.to(device), lengths

train_dataset = ModCountDataset(10000, 5, 20)
val_dataset = ModCountDataset(2000, 5, 20)
test_dataset = ModCountDataset(2000, 21, 200)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, collate_fn=collate_fn, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, collate_fn=collate_fn)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)

# %%
class PositionalEncoding(nn.Module):
    def __init__(self, embed_dim, max_len=500):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-np.log(10000.0) / embed_dim))
        pe = torch.zeros(1, max_len, embed_dim)
        pe[0,:,0::2] = torch.sin(position * div_term)
        pe[0,:,1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:,:x.size(1),:]

# %%
class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim=32, num_heads=2, num_layers=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)
        self.pos_encoding = PositionalEncoding(embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads, dropout=0.3, batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.classifier = nn.Linear(embed_dim, 2)

    def forward(self, x, lengths):
        x_embed = self.embedding(x)
        x_embed = self.pos_encoding(x_embed)
        mask = (x == PAD_IDX)
        encoder_out = self.encoder(x_embed, src_key_padding_mask=mask)
        pooled = (encoder_out * (~mask.unsqueeze(-1))).sum(1) / (~mask).sum(1, keepdim=True)
        logits = self.classifier(pooled)
        return logits, encoder_out

model = SimpleTransformer(VOCAB_SIZE).to(device)

# %%
@torch.no_grad()
def evaluate(model, loader):
    model.eval()
    total_correct = 0
    for seqs, labels, lengths in loader:
        logits, _ = model(seqs, lengths)
        total_correct += (logits.argmax(-1)==labels).sum().item()
    return total_correct / len(loader.dataset)

# %%
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

EPOCHS = 100
for epoch in range(1, EPOCHS+1):
    model.train()
    total_loss, correct = 0, 0

    for seqs, labels, lengths in train_loader:
        optimizer.zero_grad()
        logits, _ = model(seqs, lengths)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        correct += (logits.argmax(dim=-1)==labels).sum().item()

    train_acc = correct / len(train_dataset)

    # Validate every epoch
    val_acc = evaluate(model, val_loader)
    print(f"Epoch {epoch}: Train Loss {total_loss:.2f}, Train Acc {train_acc:.3f}, Val Acc {val_acc:.3f}")

#%%
test_acc = evaluate(model, test_loader)
print(f"Test accuracy (generalization): {test_acc:.3f}")

# %%
def extract_activations(model, loader):
    model.eval()
    activations, labels_list = [], []
    for seqs, labels, lengths in loader:
        _, encoder_out = model(seqs, lengths)
        pooled = encoder_out.mean(dim=1)
        activations.append(pooled.cpu())
        labels_list.append(labels.cpu())
    return torch.cat(activations), torch.cat(labels_list)

test_acts, test_labels = extract_activations(model, test_loader)

# %%
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
reduced = pca.fit_transform(test_acts.detach().numpy())

plt.scatter(reduced[:,0], reduced[:,1], c=test_labels.numpy(), cmap='coolwarm', alpha=0.7)
plt.title("PCA of Transformer Hidden States (Test Set)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.colorbar(label="innie-outie")
plt.show()
# %%
